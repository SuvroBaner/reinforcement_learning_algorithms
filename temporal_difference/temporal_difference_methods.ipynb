{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c47bd11-ff68-448e-aa40-71294fe074b1",
   "metadata": {},
   "source": [
    "#### Temporal Difference (TD) Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977eb0b6-d307-4946-90e4-866cf7fecebc",
   "metadata": {},
   "source": [
    "##### Explore Cliff Walking Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37136a6e-84ca-4177-abe0-cbb5ba96179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import defaultdict, deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "041d7a6d-9164-4473-abf6-7538feda6e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import check_test\n",
    "from plot_utils import plot_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36e4f062-d37c-4552-8b7d-ca04fee10e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CliffWalking-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475230c9-f686-4c32-a886-9e4b202dcb8b",
   "metadata": {},
   "source": [
    "The agent moves through a $4\\times 12$ gridworld, with states numbered as follows:\n",
    "```\n",
    "[[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n",
    " [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n",
    " [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n",
    " [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]]\n",
    "```\n",
    "At the start of any episode, state `36` is the initial state.  State `47` is the only terminal state, and the cliff corresponds to states `37` through `46`.\n",
    "\n",
    "The agent has 4 potential actions:\n",
    "```\n",
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3\n",
    "```\n",
    "\n",
    "Thus, $\\mathcal{S}^+=\\{0, 1, \\ldots, 47\\}$, and $\\mathcal{A} =\\{0, 1, 2, 3\\}$.  Verify this by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "751790ac-4274-4f22-9a88-4d202aa28ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n",
      "Discrete(48)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7b2714-0a4e-4028-b038-03cc8774bd1d",
   "metadata": {},
   "source": [
    "Optimal policy for the CliffWalking environment. The optimal state-value function is visualized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4437c9-08c7-435c-b55a-1a66576abeeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-kernel",
   "language": "python",
   "name": "rl-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
